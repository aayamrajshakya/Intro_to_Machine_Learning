{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGcr9GXSdJn"
      },
      "source": [
        "Name: Aayam Raj Shakya (as5160)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1 - Gradient Descent\n",
        "\n",
        "loss function: $J(\\theta_{1}, \\theta_{2}) = \\theta_{1}^{2} + 4\\theta_{2}^{2} + \\theta_{1}\\theta_{2}$\n",
        "\n",
        "initial parameters: $ \\theta_{1} = -3, \\theta_{2} = 3$\n",
        "\n",
        "Getting the partial derivatives:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_{1}} = 2\\theta_{1} + \\theta_{2}$$\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_{2}} = 8\\theta_{2} + \\theta_{1}$$\n",
        "\n",
        "Pluggint the parameter values, we get\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_{1}} = 2(-3) + (3) = -3$$\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_{2}} = 8(3) + (-3) = 21$$\n",
        "\n",
        "$$So, ∇J(-3,3) = (-3, 21)$$\n",
        "\n",
        "updated direction (flip sign) = (3, -21)\n",
        "\n",
        "*******************************************************************\n",
        "\n",
        "When learning rate $α = 0.03$\n",
        "$$\\theta_{1}^{new} = \\theta_{1} - \\alpha\\frac{\\partial J}{\\partial \\theta_{1}} = -3 - (0.03)(-3) = -2.91$$\n",
        "$$\\theta_{2}^{new} = \\theta_{2} - \\alpha\\frac{\\partial J}{\\partial \\theta_{2}} = 3 - (0.03)(21) = 2.37$$\n",
        "\n",
        "updated loss = $$J(-2.91, 2.37) = (-2.91)^{2} + 4 (2.37)^{2} + (-2.91 * 2.37) = 24.039$$\n",
        "\n",
        "********************************************************************\n",
        "\n",
        "When learning rate $α = 0.5$\n",
        "$$\\theta_{1}^{new} = \\theta_{1} - \\alpha\\frac{\\partial J}{\\partial \\theta_{1}} = -3 - (0.5)(-3) = -1.5$$\n",
        "$$\\theta_{2}^{new} = \\theta_{2} - \\alpha\\frac{\\partial J}{\\partial \\theta_{2}} = 3 - (0.5)(21) = -7.5$$\n",
        "\n",
        "updated loss = $$J(-1.5, -7.5) = (-1.5)^{2} + 4 (-7.5)^{2} + (-1.5 * -7.5) = 238.5$$"
      ],
      "metadata": {
        "id": "4Cr0_6Ea75pY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYYb_JAJSi42"
      },
      "source": [
        "## Problem 2 (15 pts) Polynomial Regression\n",
        "\n",
        "The data below (generated by code) consists of 10 samples and 2 variables (x_1, x_2), and you will use polynomial regression of degree 3 with L2 regularization to find the best-fitting equation.\n",
        "\n",
        "Please directly use the closed/analytical form in the slides for L2 regularization as the final solution. Manually tune the weight hyper parameter\n",
        "λ to improve your performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "muVd2DNcSc0v",
        "outputId": "0bb1e762-4041-4595-c908-710368ac0137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different values of the regularization term λ:\n",
            "\n",
            "\u001b[91mLambda 0.01\u001b[0m:\n",
            "1 + 0.674*X1 + 0.002*X2 + 1.674*X1^2 + 1.23*X2^2 + 0.349*X1*X2 + 1.657*X1^3 + 1.937*X2^3 + 0.314*X1*X2^2 + 0.867*X1^2*X2\n",
            "\u001b[91mLambda 0.1\u001b[0m:\n",
            "1 + 0.984*X1 + 0.631*X2 + 1.375*X1^2 + 1.13*X2^2 + 0.387*X1*X2 + 1.289*X1^3 + 1.351*X2^3 + 0.29*X1*X2^2 + 0.465*X1^2*X2\n",
            "\u001b[91mLambda 1\u001b[0m:\n",
            "1 + 0.746*X1 + 0.749*X2 + 0.743*X1^2 + 0.8*X2^2 + 0.446*X1*X2 + 0.617*X1^3 + 0.759*X2^3 + 0.323*X1*X2^2 + 0.336*X1^2*X2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate 10 samples with 2 features\n",
        "X = np.random.rand(10, 2)  # Shape: (10, 2)\n",
        "X1 = X[:, 0]; X2 = X[:, 1]\n",
        "\n",
        "# Generate target variable y with a cubic relationship\n",
        "y = 4 * X1**3 + 3 * X2**3 + 2 * X1 * X2 + np.random.randn(10) * 0.1  # Shape: (10,)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Reference: https://mathoverflow.net/q/225953\n",
        "All possible terms for polynomial of degree 3 are:\n",
        "x1, x2, x1^2, x2^2, x1x2, x1^3, x2^3, x1x2^2, x1^2x2\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Use column_stack to form a feature matrix\n",
        "a = np.array((1,2,3))\n",
        "b = np.array((2,3,4))\n",
        "np.column_stack((a,b))\n",
        ">>    array([[1, 2],\n",
        "             [2, 3],\n",
        "             [3, 4]]\n",
        "\"\"\"\n",
        "\n",
        "# I chose to add All-1 vector at leftmost\n",
        "X_new = np.column_stack((np.ones(10), X1, X2, X1**2, X2**2, X1*X2, X1**3, X2**3, X1*X2**2, X1**2*X2))\n",
        "\n",
        "# Identity matrix\n",
        "I = np.identity(10)\n",
        "\n",
        "# Setting up weight hyper-parameter\n",
        "lambda_values = [0.01 ,0.1 ,1]\n",
        "\n",
        "print(\"Different values of the regularization term λ:\\n\")\n",
        "\n",
        "# arr.T = transpose(arr)\n",
        "# matrix**(-1) = np.linalg.inv(matrix)\n",
        "# @ is used for matmul instead of asterisk '*'\n",
        "for lmbd in lambda_values:\n",
        "    theta = np.linalg.inv((X_new.T @ X_new + lmbd * I)) @ X_new.T @ y  # Shape: (10,)\n",
        "    theta = np.round(theta, 3)\n",
        "    deg_3_terms = ['1', f'{theta[1]}*X1', f'{theta[2]}*X2', f'{theta[3]}*X1^2', f'{theta[4]}*X2^2', f'{theta[5]}*X1*X2', f'{theta[6]}*X1^3', f'{theta[7]}*X2^3', f'{theta[8]}*X1*X2^2', f'{theta[9]}*X1^2*X2']\n",
        "    print(f'\\033[91mLambda {lmbd}\\033[0m:')\n",
        "    print(' + '.join(deg_3_terms))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voeyjKRaSZ-I"
      },
      "source": [
        "## Problem 3 (20 pts) L-p regularization probability\n",
        "\n",
        "Using the code above to generate data, apply L1 regularization with 3 different weights (0.01, 0.1, 1.0), respectively, to a full polynomial regression of degree 3 on two variables.\n",
        "\n",
        "Change the random seed to 123, and generate 1 dataset.\n",
        "\n",
        "For each weight, calculate the ratio of zero coefficients (including bias) in the learned model after proper training.\n",
        "\n",
        "After processing all datasets for a given weight, calculate the average ratio of zero coefficients.\n",
        "\n",
        "Finally, display the average ratio of zero coefficients for each weight, and explain the relationship between changes in weight and the zero coefficient shift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0FxCBYZ2SfW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2a94d0-747f-4a18-b8ab-92a9f4a1c6b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91mLambda 0.01:\u001b[0m\n",
            "1 + 0.8383*X1 + 0.6523*X2 + 1.2102*X1^2 + 0.5906*X2^2 + 0.8654*X1*X2 + 1.3525*X1^3 + 0.4375*X2^3 + 0.6304*X1*X2^2 + 0.9351*X1^2*X2\n",
            "Number of coeffs reduced to zero: 0\n",
            "Ratio of zero coeffs: 0.0\n",
            "\n",
            "\u001b[91mLambda 0.1:\u001b[0m\n",
            "1 + 0.8881*X1 + 0.65*X2 + 1.1106*X1^2 + 0.3001*X2^2 + 0.6093*X1*X2 + 1.1688*X1^3 + 0.001*X2^3 + 0.1583*X1*X2^2 + 0.5643*X1^2*X2\n",
            "Number of coeffs reduced to zero: 1\n",
            "Ratio of zero coeffs: 0.1\n",
            "\n",
            "\u001b[91mLambda 1.0:\u001b[0m\n",
            "1 + 0.0003*X1 + 0.0135*X2 + 0.0084*X1^2 + -0.002*X2^2 + -0.0035*X1*X2 + 0.0016*X1^3 + 0.0041*X2^3 + 0.0036*X1*X2^2 + 0.0096*X1^2*X2\n",
            "Number of coeffs reduced to zero: 8\n",
            "Ratio of zero coeffs: 0.8\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "# Generate 10 samples with 2 features\n",
        "X = np.random.rand(10, 2)  # Shape: (10, 2)\n",
        "X1 = X[:, 0]; X2 = X[:, 1]\n",
        "\n",
        "# Generate target variable y with a cubic relationship\n",
        "y = 4 * X1**3 + 3 * X2**3 + 2 * X1 * X2 + np.random.randn(10) * 0.1  # Shape: (10,)\n",
        "\n",
        "X_new = np.column_stack((np.ones(10), X1, X2, X1**2, X2**2, X1*X2, X1**3, X2**3, X1*X2**2, X1**2*X2))\n",
        "\n",
        "# Weights list\n",
        "wgts_list = [0.01, 0.1, 1.0]\n",
        "\n",
        "def l1_regularization(X, y, lambda_l1, learning_rate=0.01, epochs=1000):\n",
        "  weights = np.zeros(X.shape[0])\n",
        "  m = len(X)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    y_pred = X @ weights\n",
        "    dw = (1/m) * X.T @ (y_pred - y)\n",
        "    weights = weights - learning_rate * (dw + lambda_l1 * np.sign(weights))\n",
        "  return weights\n",
        "\n",
        "for lmbd in wgts_list:\n",
        "  wgt = l1_regularization(X_new, y, lmbd)\n",
        "  print(f\"\\033[91mLambda {lmbd}:\\033[0m\")\n",
        "  wgt = np.round(wgt, 4)\n",
        "\n",
        "  # Counting the no. of coefficients that fall below 0.001, ignoring the bias term\n",
        "  zero_coeff_cnt = np.sum(wgt[1:] <= 1e-2)\n",
        "\n",
        "  deg_3_terms = ['1', f'{wgt[1]}*X1', f'{wgt[2]}*X2', f'{wgt[3]}*X1^2', f'{wgt[4]}*X2^2', f'{wgt[5]}*X1*X2', f'{wgt[6]}*X1^3', f'{wgt[7]}*X2^3', f'{wgt[8]}*X1*X2^2', f'{wgt[9]}*X1^2*X2']\n",
        "  print(' + '.join(deg_3_terms))\n",
        "\n",
        "  print(f\"Number of coeffs reduced to zero: {zero_coeff_cnt}\")\n",
        "  print(f\"Ratio of zero coeffs: {zero_coeff_cnt / len(wgt)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The larger the regularization term, the greater its strength. Hence, more coefficients are minimized and pushed toward negligibility"
      ],
      "metadata": {
        "id": "nmZq_mDt3Af8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "usr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}